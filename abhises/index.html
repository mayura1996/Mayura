
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Zip-NeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://jonbarron.info/zipnerf/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/zipnerf/"/>
    <meta property="og:title" content="Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields" />
    <meta property="og:description" content="Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8%-77% lower than either prior technique, and that trains 24x faster than mip-NeRF 360." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields" />
    <meta name="twitter:description" content="Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8%-77% lower than either prior technique, and that trains 24x faster than mip-NeRF 360." />
    <meta name="twitter:image" content="https://jonbarron.info/zipnerf/img/teaser.jpg" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>âš¡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Predictive Analysis of Accidents Based on
                    US Accident Data</b></br> 
                <small>
                    2023 14th International Conference on Information and Communication Technology Convergence (ICTC)
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="../index.html">
                          Mayura Manawadu
                        </a>
                    </li>
                    <li>
                        <a href="https://sites.google.com/view/udayawijenayake/">
                            Udaya Wijenayake
                        </a>
                    </li>
            
                    </br>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://drive.google.com/file/d/1YMatmlC94-OO9pHcp4PGCYxBH4s1QVmD/view?usp=sharing">
                            <image src="img/abhisespaper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/syQGJfk4cGQ">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="https://github.com/jonbarron/camp_zipnerf">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                        <!-- <li>
                            <a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_camp_zipnerf.ipynb">
                            <image src="img/VertexAI-512-color.webp" height="60px">
                                <h4><strong>Google Vertex AI Notebook</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/3VhgjcyQQ4Q?si=njgaHkGI8sTxaDAl&amp;start=113" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
<br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Abhises is an innovative AR virtual tour guide designed to make visiting tourist sites like Sigiriya in Sri Lanka more interactive and enjoyable. It uses advanced computer vision and AR technology to guide users around, offering detailed explanations and answering questions in real-time, just like a human guide but with more convenience and flexibility. The system is smart enough to interact with users even while it's speaking, creating a more engaging experience. It's designed with human-like features for a more relatable guide, and its technology can be adapted for any tourist site, aiming to make solo tours not just easier, but also more informative and fun.               </p>
            </div>
        </div>

<!-- 
       
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Multisampling
                </h3>
				<table style="width: 100%; border-collapse: collapse;">
				  <tr>
				    <td style="text-align: center;">
		                <video id="v0" width="100%" autoplay loop muted>
		                  <source src="img/hexify_train.mp4" type="video/mp4" />
		                </video>
					</td>
				    <td style="text-align: center;">
		                <video id="v0" width="100%" autoplay loop muted>
		                  <source src="img/hexify_test.mp4" type="video/mp4" />
		                </video>
					</td>
				  </tr>
				  <tr>
				    <td style="text-align: center;">When Training</td>
				    <td style="text-align: center;">When Rendering</td>
				  </tr>
				</table>
                <p class="text-justify">
                    We use multisampling to approximate the average NGP feature over a conical frustum, by constructing a 6-sample pattern that exactly matches the frustum's first and second moments. When training, we randomly rotate and flip (along the ray axis) each pattern, and when rendering we deterministically flip and rotate each adjacent pattern by 30 degrees.
                </p>
            </div>
        </div>
<br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    XY aliasing
                </h3>
                <video class="video" width=100% id="xyalias" loop playsinline autoplay muted src="img/xy_alias_swipe_crf27.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas height=0 class="videoMerge" id="xyaliasMerge"></canvas>
                <p class="text-justify">
                    A naive baseline (left) combining mip-NeRF 360 and Instant NGP results in aliasing as the camera moves laterally. Our full method (right) produces prefiltered renderings that do not flicker or shimmer.
                </p>
            </div>
        </div>
           
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Z aliasing
                </h3>
                <video id="v0" width="100%" autoplay loop muted>
                  <source src="img/z_alias_pdf_labeled.m4v" type="video/mp4" />
                </video>
                <p class="text-justify">
                    The proposal network used for resampling points along rays in mip-NeRF 360 results in an artifact we refer to as <em>z-aliasing</em>, where foreground content alternately appears and disappears as the camera moves toward or away from scene content. Z-aliasing occurs when the initial set of samples from the proposal network is not dense enough and misses thin structures, such as the chair above. Missed content can not be recovered by later rounds of sampling, since no future samples will be placed at that location along the ray. Our improvements to proposal network supervision result in a prefiltered proposal output that preserves the foreground object for all frames in this sequence. The plots above depict samples along a ray for three rounds of resampling (blue, orange, and green lines), with the y axis showing rendering weight (how much each interval contributes to the final rendered color), as a normalized probability density.
                </p>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{manawadu2023predictive,
        title={Predictive Analysis of Accidents Based on US Accident Data},
        author={Manawadu, Mayura and Wijenayake, Udaya},
        booktitle={2023 14th International Conference on Information and Communication Technology Convergence (ICTC)},
        pages={199--204},
        year={2023},
        organization={IEEE}
}
</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
This research was supported by the Science and
Technology Human Resource Development Project, Ministry
of Education, Sri Lanka, funded by the Asian Development
Bank (Grant No. STHRD/CRG/R1/SJ/06).                    <br><br>
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
